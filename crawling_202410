import requests
from bs4 import BeautifulSoup
from apify_client import ApifyClient

# Apifyクライアントの初期化
client = ApifyClient('ここにはapifyのクライアントキーを入れる')

async def main() -> None:

    # プレスリリースページのURL
    url = "https://prtimes.jp/technology/"


    # ページをリクエスト
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # プレスリリース情報を抽出
    press_releases = []
    for article in soup.select('article.list-article'):
        title = article.select_one('h3.list-article__title').get_text(strip=True)
        company = article.select_one('span.list-article__company-name--dummy').get_text(strip=True)
        date = article.select_one('time').get('datetime')
        link = article.select_one('a').get('href')

        press_releases.append({
            'title': title,
            'company': company,
            'date': date,
            'link': f"https://prtimes.jp{link}"
        })

    # 結果を出力
    for release in press_releases:
        print(f"Title: {release['title']}")
        print(f"Company: {release['company']}")
        print(f"Date: {release['date']}")
        print(f"Link: {release['link']}")
        print("------")

    # 取得したデータをApifyのデータセットに保存
    run = client.actor('apify/hello-world').call()
    client.dataset(run['defaultDatasetId']).push_items(press_releases)
